{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, MaxPool2D,\n",
    "    Dense, Dropout, Flatten\n",
    ")\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "tf.random.set_seed(random_state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Using CPU for computation.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available. Using GPU for computation.\")\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU for computation.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    batch_size = 5000\n",
    "    dataframe = pd.read_csv(file_path)\n",
    "\n",
    "    texts = dataframe['content'].tolist()\n",
    "    labels = dataframe['majority_voting'].tolist()\n",
    "    original_labels = dataframe['label'].tolist()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    num_labels = len(labels)\n",
    "    num_batches = (num_labels + batch_size - 1) // batch_size\n",
    "\n",
    "    all_labels = []\n",
    "    all_original_labels = []\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_original_labels = original_labels[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        encoded_labels = label_encoder.fit_transform(batch_labels)\n",
    "        encoded_original_labels = label_encoder.fit_transform(batch_original_labels)\n",
    "\n",
    "        all_labels.append(encoded_labels)\n",
    "        all_original_labels.append(encoded_original_labels)\n",
    "\n",
    "        gc.collect()\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    return texts, np.concatenate(all_labels, axis=0), np.concatenate(all_original_labels, axis=0)\n",
    "\n",
    "texts, labels, original_labels = load_and_preprocess_data('datasets/ISOT/merged/dataset_labeled.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/225 [09:25<8:41:10, 141.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 39\u001B[0m\n\u001B[0;32m     35\u001B[0m             tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mbackend\u001B[38;5;241m.\u001B[39mclear_session()\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mconcatenate(all_embeddings, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 39\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mextract_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, embeddings\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[1;32mIn[5], line 25\u001B[0m, in \u001B[0;36mextract_embeddings\u001B[1;34m(texts, batch_size)\u001B[0m\n\u001B[0;32m     17\u001B[0m encodings \u001B[38;5;241m=\u001B[39m tokenizer(\n\u001B[0;32m     18\u001B[0m     batch_texts,\n\u001B[0;32m     19\u001B[0m     truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     20\u001B[0m     padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     21\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     22\u001B[0m )\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 25\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencodings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencodings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m     batch_embeddings \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     31\u001B[0m all_embeddings\u001B[38;5;241m.\u001B[39mappend(batch_embeddings)\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:798\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_sdpa \u001B[38;5;129;01mand\u001B[39;00m head_mask_is_none \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m output_attentions:\n\u001B[0;32m    794\u001B[0m         attention_mask \u001B[38;5;241m=\u001B[39m _prepare_4d_attention_mask_for_sdpa(\n\u001B[0;32m    795\u001B[0m             attention_mask, embeddings\u001B[38;5;241m.\u001B[39mdtype, tgt_len\u001B[38;5;241m=\u001B[39minput_shape[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    796\u001B[0m         )\n\u001B[1;32m--> 798\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    803\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    805\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:551\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    543\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    544\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    545\u001B[0m         hidden_state,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    548\u001B[0m         output_attentions,\n\u001B[0;32m    549\u001B[0m     )\n\u001B[0;32m    550\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 551\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    558\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    560\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:495\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001B[0m\n\u001B[0;32m    492\u001B[0m sa_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msa_layer_norm(sa_output \u001B[38;5;241m+\u001B[39m x)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;66;03m# Feed Forward Network\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m ffn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mffn\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa_output\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[0;32m    496\u001B[0m ffn_output: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_layer_norm(ffn_output \u001B[38;5;241m+\u001B[39m sa_output)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[0;32m    498\u001B[0m output \u001B[38;5;241m=\u001B[39m (ffn_output,)\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:429\u001B[0m, in \u001B[0;36mFFN.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 429\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mff_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[1;32m--> 248\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:434\u001B[0m, in \u001B[0;36mFFN.ff_chunk\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    432\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin1(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    433\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(x)\n\u001B[1;32m--> 434\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    435\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m    436\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def extract_embeddings(texts, batch_size=200):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "        'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "    )\n",
    "    model = DistilBertModel.from_pretrained(\n",
    "        'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "    )\n",
    "\n",
    "    num_texts = len(texts)\n",
    "    num_batches = (num_texts + batch_size - 1) // batch_size\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        batch_texts = texts[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=encodings['input_ids'],\n",
    "                attention_mask=encodings['attention_mask']\n",
    "            )\n",
    "            batch_embeddings = outputs.last_hidden_state.numpy()\n",
    "\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n",
    "        gc.collect()\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "embeddings = extract_embeddings(texts)\n",
    "print(\"Embedding shape:\", embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(256, (3, 3), strides=1, padding=\"same\", activation=\"relu\",\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(256, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"valid\"))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(128, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"valid\"))\n",
    "\n",
    "    # Fourth Convolutional Block\n",
    "    model.add(Conv2D(128, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"valid\"))\n",
    "\n",
    "    # Dense Layers\n",
    "    model.add(Dense(units=256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=128, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/sequential_1/batch_normalization_2_1/moments/BroadcastTo defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Python311\\Lib\\asyncio\\base_events.py\", line 604, in run_forever\n\n  File \"C:\\Python311\\Lib\\asyncio\\base_events.py\", line 1909, in _run_once\n\n  File \"C:\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\pc cam\\AppData\\Local\\Temp\\ipykernel_21996\\2384889723.py\", line 80, in <module>\n\n  File \"C:\\Users\\pc cam\\AppData\\Local\\Temp\\ipykernel_21996\\2384889723.py\", line 32, in cross_validate\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 70, in train_step\n\nOOM when allocating tensor with shape[8,256,384,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node gradient_tape/sequential_1/batch_normalization_2_1/moments/BroadcastTo}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_5456]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 80\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;66;03m# Perform cross-validation\u001B[39;00m\n\u001B[0;32m     79\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(labels))\n\u001B[1;32m---> 80\u001B[0m metrics, results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[59], line 32\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(X, y, num_classes, k_folds)\u001B[0m\n\u001B[0;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m create_cnn_model(\n\u001B[0;32m     27\u001B[0m     input_shape\u001B[38;5;241m=\u001B[39m(X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], \u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     28\u001B[0m     num_classes\u001B[38;5;241m=\u001B[39mnum_classes\n\u001B[0;32m     29\u001B[0m )\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m     38\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Predict\u001B[39;00m\n\u001B[0;32m     41\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_val)\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     54\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node gradient_tape/sequential_1/batch_normalization_2_1/moments/BroadcastTo defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Python311\\Lib\\asyncio\\base_events.py\", line 604, in run_forever\n\n  File \"C:\\Python311\\Lib\\asyncio\\base_events.py\", line 1909, in _run_once\n\n  File \"C:\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\pc cam\\AppData\\Local\\Temp\\ipykernel_21996\\2384889723.py\", line 80, in <module>\n\n  File \"C:\\Users\\pc cam\\AppData\\Local\\Temp\\ipykernel_21996\\2384889723.py\", line 32, in cross_validate\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"C:\\Users\\pc cam\\Desktop\\FND\\paper PFE 2023\\UFNAC\\ufnac_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 70, in train_step\n\nOOM when allocating tensor with shape[8,256,384,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node gradient_tape/sequential_1/batch_normalization_2_1/moments/BroadcastTo}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_5456]"
     ]
    }
   ],
   "source": [
    "def cross_validate(X, y, num_classes, original_labels, k_folds=5):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "\n",
    "    detailed_results = []\n",
    "\n",
    "    # Reshape embeddings for 2D convolution\n",
    "    X_2d = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_2d, y), 1):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        X_train, X_val = X_2d[train_idx], X_2d[val_idx]\n",
    "        y_train, y_val = y[train_idx], original_labels[val_idx]\n",
    "\n",
    "        model = create_cnn_model(\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2], 1),\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred_classes)\n",
    "        precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "        f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "        fold_metrics['accuracy'].append(accuracy)\n",
    "        fold_metrics['precision'].append(precision)\n",
    "        fold_metrics['recall'].append(recall)\n",
    "        fold_metrics['f1_score'].append(f1)\n",
    "\n",
    "        detailed_results.append({\n",
    "            'fold': fold,\n",
    "            'y_true': y_val,\n",
    "            'y_pred': y_pred_classes,\n",
    "            'metrics': {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "        })\n",
    "\n",
    "        print(classification_report(y_val, y_pred_classes))\n",
    "\n",
    "    print(\"\\nCross-Validation Summary:\")\n",
    "    for metric, values in fold_metrics.items():\n",
    "        print(f\"{metric.capitalize()} - Mean: {np.mean(values):.4f}, Std: {np.std(values):.4f}\")\n",
    "\n",
    "    return fold_metrics, detailed_results\n",
    "\n",
    "num_classes = len(np.unique(labels))\n",
    "metrics, results = cross_validate(embeddings, labels, num_classes, original_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_cross_validation_results(metrics):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i, (metric, values) in enumerate(metrics.items(), 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        plt.bar(range(1, len(values) + 1), values)\n",
    "        plt.title(f\"{metric.capitalize()} per Fold\")\n",
    "        plt.xlabel(\"Fold\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cross_validation_results(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
